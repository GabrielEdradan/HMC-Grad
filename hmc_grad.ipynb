{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z2oy94RRxJa"
      },
      "source": [
        "# HMC-GRAD: HANDWRITTEN MULTIPLE-CHOICE TEST GRADER\n",
        "HMC-Grad grades answer sheets of multiple-choice tests handwritten on yellow notepads given a correction key. It employs OpenCV for image processing and PyTorch for training a convolutional neural network on the EMNIST dataset for image classification of letters A to D.\n",
        "\n",
        "Testing shows a mean grading accuracy of 94.75%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaklWdMZRwYU"
      },
      "source": [
        "## IMPORT MODULES AND SET GLOBALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5Zz7NinfDDM"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4U_dUaQs2i-"
      },
      "outputs": [],
      "source": [
        "# For downloading model weights\n",
        "import os\n",
        "\n",
        "# For image preprocessing\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# For the image classifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# For the interface\n",
        "import gradio as gr\n",
        "\n",
        "# For formatting data\n",
        "import pandas as pd\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrOzL7g0zab_"
      },
      "outputs": [],
      "source": [
        "# Globals Constants:\n",
        "\n",
        "# For preprocessing\n",
        "IMAGE_WIDTH = 1125\n",
        "IMAGE_HEIGHT = 1500\n",
        "THRESH_BLOCK_SIZE = 11\n",
        "THRESH_CONSTANT = 5\n",
        "LINE_LENGTH = 5000\n",
        "\n",
        "# For ROI extraction\n",
        "MAX_COMPONENT_MERGE_DISTANCE = 30\n",
        "MIN_COMPONENT_SIDE = 15\n",
        "Y_EPSILON = 25\n",
        "NUMBER_OF_COLUMNS = 2\n",
        "\n",
        "# For image classification\n",
        "PREDICTION_TO_STRING = [\"A\", \"B\", \"C\", \"D\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFv3JBjAOy08"
      },
      "source": [
        "## THE CLASSIFIER\n",
        "Prepare the Classifier Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK9WDGQQoKOA"
      },
      "outputs": [],
      "source": [
        "# Download the model's state dictionary from repository\n",
        "GITHUB_URL = \"https://raw.githubusercontent.com/GabrielEdradan/HMC-Grad/main/image_classifier.pth\"\n",
        "MODEL_PATH = \"/content/model.pth\"\n",
        "os.system(f\"wget {GITHUB_URL} -O {MODEL_PATH}\")\n",
        "model_state_dict = torch.load(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avmc5ro9s1s1"
      },
      "outputs": [],
      "source": [
        "# Define the model class\n",
        "class ABCDClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ABCDClassifier, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(32 * 4 * 4, 128)\n",
        "    self.fc2 = nn.Linear(128, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 32 * 4 * 4)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return F.softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3XUKncaOrDJ"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = ABCDClassifier()\n",
        "model.load_state_dict(model_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsWpajjjOtmT"
      },
      "source": [
        "## THE IMAGE PROCESSOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q5HWgmSQx4t"
      },
      "source": [
        "### The Image Processor Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaVWpmkRtXFN"
      },
      "outputs": [],
      "source": [
        "# Define the main function used for the interface.\n",
        "# Takes in an array of strings (image paths), and an array of chars (correction key)\n",
        "# Returns six str file paths to files: correctness (csv), scores (csv), item analysis (csv), score analysis (csv), merged data (xlsx), and corrected images (zip of jpgs)\n",
        "def process_image_set(image_paths_array, correction_key):\n",
        "  correctness = [] # an array of binary int arrays, of length len(image_paths_array)\n",
        "  scores = [] # an array of ints, of length len(image_paths_array)\n",
        "  item_analysis = [0] * len(correction_key) # an array of ints\n",
        "  score_analysis = [0] * (len(correction_key) + 1) # an array of ints\n",
        "  corrected_images = [] # An array of OpenCV loaded images (matrices)\n",
        "\n",
        "  for i in range(len(image_paths_array)):\n",
        "    process_image(image_paths_array[i], correctness, scores, item_analysis, score_analysis, correction_key, corrected_images)\n",
        "\n",
        "  # --------------------------------------------------------------------------------------------------------------------\n",
        "  # For Formatting output data\n",
        "  # Define csv file paths\n",
        "  student_correctness_csv_path = 'student_correctness.csv'\n",
        "  student_scores_csv_path = 'student_scores.csv'\n",
        "  item_analysis_csv_path = 'item_analysis.csv'\n",
        "  score_analysis_csv_path = 'score_analysis.csv'\n",
        "  merged_xlsx_path = 'merged_data.xlsx'\n",
        "  # Define zip file path\n",
        "  corrected_images_zip_file_path = 'corrected_images.zip'\n",
        "\n",
        "  # ----------------------------------------------------------------\n",
        "  # For the CSV files\n",
        "  # For correctness\n",
        "  transposed_data = list(map(list, zip(*correctness))) # Transpose the data to have students as columns\n",
        "  columns = [f\"Student {i+1}\" for i in range(len(correctness))] # Define the columns\n",
        "  correctness_df = pd.DataFrame(transposed_data, columns=columns) # Create the DataFrame\n",
        "  correctness_df.insert(0, 'Item Number', range(1, len(correctness[0]) + 1)) # Add the item number column\n",
        "  correctness_df.to_csv(student_correctness_csv_path, index=False) # Save\n",
        "\n",
        "  # For student scores\n",
        "  columns = [\"Score\"] # Define the columns\n",
        "  scores_df = pd.DataFrame(scores, columns=columns) # Create the DataFrame\n",
        "  scores_df.insert(0, 'Student Number', range(1, len(scores) + 1)) # Add the student number column\n",
        "  scores_df.to_csv(student_scores_csv_path, index=False) # Save\n",
        "\n",
        "  # For item analysis\n",
        "  columns = [\"Number of Correct Answers\"] # Define the columns\n",
        "  item_analysis_df = pd.DataFrame(item_analysis, columns=columns) # Create the DataFrame\n",
        "  item_analysis_df.insert(0, 'Item Number', range(1, len(item_analysis) + 1)) # Add the student number column\n",
        "  item_analysis_df.to_csv(item_analysis_csv_path, index=False) # Save\n",
        "\n",
        "  # For score analysis\n",
        "  columns = [\"Number of Students\"] # Define the columns\n",
        "  score_analysis_df = pd.DataFrame(score_analysis, columns=columns) # Create the DataFrame\n",
        "  score_analysis_df.insert(0, 'Score', range(0, len(score_analysis))) # Add the student number column\n",
        "  score_analysis_df.to_csv(score_analysis_csv_path, index=False) # Save\n",
        "\n",
        "  # ----------------------------------------------------------------\n",
        "  # For merging the CSV files into XLSX\n",
        "  # Create a writer to save multiple dataframes to a single XLSX file\n",
        "  with pd.ExcelWriter(merged_xlsx_path) as writer:\n",
        "      # Write each dataframe to a different sheet\n",
        "      correctness_df.to_excel(writer, sheet_name='Correctness', index=False)\n",
        "      scores_df.to_excel(writer, sheet_name='Scores', index=False)\n",
        "      item_analysis_df.to_excel(writer, sheet_name='Item Analysis', index=False)\n",
        "      score_analysis_df.to_excel(writer, sheet_name='Score Analysis', index=False)\n",
        "\n",
        "  # ----------------------------------------------------------------\n",
        "  # For the ZIP file\n",
        "  # Compress corrected images into a zip file\n",
        "  with zipfile.ZipFile(corrected_images_zip_file_path, 'w') as zipf:\n",
        "    for i, image in enumerate(corrected_images):\n",
        "      image_filename = f\"image_{i}.jpg\"  # You can change the extension based on your image format\n",
        "      cv2.imwrite(image_filename, image)  # Save the image temporarily\n",
        "      zipf.write(image_filename)  # Add the image to the zip file\n",
        "      os.remove(image_filename)  # Remove the temporary image file\n",
        "\n",
        "  return student_correctness_csv_path, student_scores_csv_path, item_analysis_csv_path, score_analysis_csv_path, merged_xlsx_path, corrected_images_zip_file_path\n",
        "\n",
        "\n",
        "# Define a helper function for readability\n",
        "# Takes in an image path, the four arrays to be modified, an array of chars, and an array of matrices (to be modified too)\n",
        "# Void return value, modifies the four arrays directly\n",
        "def process_image(img_path, img_cor_arr, img_scr_arr, itm_ana_arr, scr_ana_arr, correction_key, corrected_images):\n",
        "  base_image = cv2.imread(img_path)\n",
        "\n",
        "  # For Preprocessing\n",
        "  segmentation_image, ocr_image, processing_error = preprocess(base_image)\n",
        "  if processing_error: # Check for exception\n",
        "    invalid_num_of_items(\"PROCESSING ERROR\", img_cor_arr, img_scr_arr, correction_key, ocr_image, corrected_images)\n",
        "    return\n",
        "\n",
        "  # For ROI Extraction\n",
        "  roi_bounds, extraction_error = extract_rois(segmentation_image, len(correction_key), ocr_image)\n",
        "  if extraction_error: # Check for exception\n",
        "    invalid_num_of_items(\"EXTRACTION ERROR\", img_cor_arr, img_scr_arr, correction_key, ocr_image, corrected_images)\n",
        "    return\n",
        "\n",
        "  # For Classification\n",
        "  item_answers = classify_rois(roi_bounds, ocr_image)\n",
        "\n",
        "  if len(item_answers) != len(correction_key): # Check for exception (extra layer of safety)\n",
        "    invalid_num_of_items(\"UNMATCHING ITEM NUMBER\", img_cor_arr, img_scr_arr, correction_key, ocr_image, corrected_images)\n",
        "    return\n",
        "\n",
        "  # For Grading and Analysis\n",
        "  corrected_image = grade_and_analyze(item_answers, img_cor_arr, img_scr_arr, itm_ana_arr, scr_ana_arr, correction_key, ocr_image, roi_bounds)\n",
        "  corrected_images.append(corrected_image)\n",
        "\n",
        "\n",
        "# Function for error logic: modifies relevant arrays and adds a custom annotated image that indicates the error\n",
        "# Takes in an error string, the arrays to be modified, the correction key, the ocr image, and the array of corrected images\n",
        "# Void return value, modifies the arrays directly\n",
        "def invalid_num_of_items(error_string, img_cor_arr, img_scr_arr, correction_key, ocr_image, corrected_images):\n",
        "  print(f\"ERROR: {error_string}\")\n",
        "  corrected_image = grade_invalid_paper(img_cor_arr, img_scr_arr, correction_key, ocr_image, error_string)\n",
        "  corrected_images.append(corrected_image)\n",
        "\n",
        "\n",
        "# Function to be called to invalidate a paper\n",
        "# Takes in 2 arrays (image correction and scores) to be modified, the correction key, the ocr image, and an error string\n",
        "# Returns the corrected image\n",
        "def grade_invalid_paper(img_cor_arr, img_scr_arr, correction_key, ocr_image, error_string):\n",
        "  corrected_image = np.copy(ocr_image)\n",
        "  corrected_image = cv2.cvtColor(corrected_image, cv2.COLOR_GRAY2BGR)\n",
        "  correction_array = []\n",
        "  score = -1\n",
        "\n",
        "  for i in range(len(correction_key)):\n",
        "    correction_array.append(-1)\n",
        "\n",
        "  # Write a score on the upper right\n",
        "  cv2.putText(corrected_image, \"INVALID\", (IMAGE_WIDTH - 300, 200), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)\n",
        "  cv2.putText(corrected_image, error_string, (IMAGE_WIDTH - 350, 250), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "  # Bring changes to the input arrays (except item analysis, which was updated during the loop)\n",
        "  img_cor_arr.append(correction_array)\n",
        "  img_scr_arr.append(score)\n",
        "  return corrected_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwhaA5O4QsDv"
      },
      "source": [
        "### Image Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzWGfdPG58Cb"
      },
      "outputs": [],
      "source": [
        "# Define the preprocess function, which performs preprocessing on the base image for ROI extraction\n",
        "# Takes in an image (the base image)\n",
        "# Returns an image for segmentation, an image for OCR, and a boolean for error handling\n",
        "def preprocess(input_image):\n",
        "  # --------------------------------------------------------------------------------------------------------------------\n",
        "  # For the OCR image\n",
        "  resized_image = cv2.resize(input_image, (IMAGE_WIDTH, IMAGE_HEIGHT)) # MOVED FROM INTERFACE TO HERE\n",
        "  gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "  threshed_gray = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, THRESH_BLOCK_SIZE, THRESH_CONSTANT)\n",
        "  opened = cv2.bitwise_not(cv2.morphologyEx(cv2.bitwise_not(threshed_gray), cv2.MORPH_OPEN, np.ones((2,2),np.uint8), iterations=1))\n",
        "  ocr_image = opened\n",
        "\n",
        "  # --------------------------------------------------------------------------------------------------------------------\n",
        "  # For the header mask\n",
        "  # Perform initial processing\n",
        "  blur = cv2.GaussianBlur(gray, (9,9), 0)\n",
        "  threshed_blur = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, THRESH_BLOCK_SIZE, THRESH_CONSTANT)\n",
        "  removed_lines = remove_lines(threshed_blur, 6)\n",
        "\n",
        "  # Determine divider y positions\n",
        "  intensely_dilated = cv2.dilate(cv2.bitwise_not(removed_lines), cv2.getStructuringElement(cv2.MORPH_RECT, (2000, 40)), iterations=1)\n",
        "  black_ys = np.where(np.any(intensely_dilated == 0, axis=1))[0].tolist()\n",
        "  to_be_removed = []\n",
        "  for i in range(1, len(black_ys)):\n",
        "    if black_ys[i] - black_ys[i - 1] == 1:\n",
        "      to_be_removed.append(i)\n",
        "  stripe_positions = [black_ys[idx] for idx in range(len(black_ys)) if not idx in to_be_removed]\n",
        "\n",
        "  # Determine which stripe is the header border\n",
        "  num_of_stripe_positions = len(stripe_positions)\n",
        "  header_border = 0\n",
        "  match num_of_stripe_positions:\n",
        "    case 1:\n",
        "      header_border = stripe_positions[0]\n",
        "    case 2:\n",
        "      header_border = stripe_positions[0] if stripe_positions[1] > 500 else stripe_positions[1]\n",
        "    case _:\n",
        "      found = False\n",
        "      for stripe_position in stripe_positions:\n",
        "        if stripe_position > 100 and stripe_position < 500:\n",
        "          header_border = stripe_position\n",
        "          found = True\n",
        "          continue\n",
        "      if not found: # If there is no stripe, return the image with header, and raise a true bool flag\n",
        "        return cv2.dilate(cv2.bitwise_not(removed_lines), cv2.getStructuringElement(cv2.MORPH_RECT, (10, 6)), iterations=1), ocr_image, True\n",
        "\n",
        "  # Create a mask based on header border\n",
        "  mask = np.ones(intensely_dilated.shape, dtype=np.uint8) * 255\n",
        "  mask[header_border:, :] = 0\n",
        "\n",
        "  # --------------------------------------------------------------------------------------------------------------------\n",
        "  # For the segmentation image\n",
        "  masked_removed_lines = cv2.bitwise_or(removed_lines, mask)\n",
        "  segmentation_image = cv2.dilate(cv2.bitwise_not(masked_removed_lines), cv2.getStructuringElement(cv2.MORPH_RECT, (10, 6)), iterations=1)\n",
        "\n",
        "  return segmentation_image, ocr_image, False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxn1iQUX7be6"
      },
      "outputs": [],
      "source": [
        "# Define a helper function for preprocessing that removes the horizontal lines in an image\n",
        "# Takes in an image, and optionally line thickness (an int that determines how erased the notepad lines should be)\n",
        "# Returns a modified image, where the notepad lines are (or have been attempted to be) erased\n",
        "def remove_lines(input_image, thickness=2):\n",
        "  edges = cv2.Canny(input_image, 200, 200)\n",
        "  lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)\n",
        "\n",
        "  # Check for when lines are not found\n",
        "  if lines is None:\n",
        "    return input_image\n",
        "  if len(lines) == 0:\n",
        "    return input_image\n",
        "\n",
        "  # Create a mask based on the lines found by HoughLines\n",
        "  line_mask = np.zeros_like(input_image)\n",
        "  for line in lines:\n",
        "    rho, theta = line[0]\n",
        "    if theta < 1.0 or theta > 2.0:\n",
        "      continue\n",
        "    a = np.cos(theta)\n",
        "    b = np.sin(theta)\n",
        "    x0 = a * rho\n",
        "    y0 = b * rho\n",
        "    x1 = int(x0 + LINE_LENGTH * (-b))\n",
        "    y1 = int(y0 + LINE_LENGTH * (a))\n",
        "    x2 = int(x0 - LINE_LENGTH * (-b))\n",
        "    y2 = int(y0 - LINE_LENGTH * (a))\n",
        "    cv2.line(line_mask, (x1, y1), (x2, y2), (255, 255, 255), 2)\n",
        "\n",
        "  # Dilate the lines vertically based on thickness parameter\n",
        "  dilation_kernel = np.ones((thickness, 1),np.uint8)\n",
        "  line_mask = cv2.dilate(line_mask, dilation_kernel, iterations=1)\n",
        "\n",
        "  # Subtract the mask from the base image and applies MORPH_OPEN to denoise\n",
        "  sub_result = cv2.bitwise_or(input_image, line_mask)\n",
        "  final_result = cv2.bitwise_not(cv2.morphologyEx(cv2.bitwise_not(sub_result), cv2.MORPH_OPEN, np.ones((2, 2),np.uint8), iterations=1))\n",
        "  return final_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61Yx4uiBQ4K0"
      },
      "source": [
        "### Region of Interest Extraction Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgIH1Imp1EQ4"
      },
      "outputs": [],
      "source": [
        "# Define the extract ROIs function, that determines the bounds of the letters in the answer sheet\n",
        "# Takes in an image for segmentation, an image for OCR, and the target number of ROIs\n",
        "# Returns an array of ROI bounds (arrays like [x, y, h, w]) and a boolean for error handling\n",
        "def extract_rois(segmentation_image, num_of_items, ocr_image): # DEBUG: REMOVE OCR IMAGE\n",
        "  # Get the components in the input image\n",
        "  _, _, stats, centroids = cv2.connectedComponentsWithStats(segmentation_image, connectivity=4)\n",
        "\n",
        "  # Define all bounds (stats excluding area)\n",
        "  all_bounds = [stat[:4].tolist() for stat in stats]\n",
        "\n",
        "  # Remove the background from bounds and centroids\n",
        "  all_bounds.pop(0)\n",
        "  centroids = centroids.tolist()\n",
        "  centroids.pop(0)\n",
        "\n",
        "  # Find components that are close to each other and merge them\n",
        "  nearby_component_pairs = find_nearby_pairs(centroids, MAX_COMPONENT_MERGE_DISTANCE)\n",
        "  mergeable_components = find_mergeable_components(nearby_component_pairs)\n",
        "  merged_bounds = merge_groups(mergeable_components, all_bounds)\n",
        "\n",
        "  # Make an array of bounds, exclude bounds that were used in merging and bounds that are too small\n",
        "  roi_bounds = [bound[:4] for index, bound in enumerate(all_bounds) if not any(index in group for group in mergeable_components) and (bound[2] > MIN_COMPONENT_SIDE and bound[3] > MIN_COMPONENT_SIDE)]\n",
        "\n",
        "  # Add the merged bounds\n",
        "  roi_bounds.extend(merged_bounds)\n",
        "\n",
        "  # KMeans needs at least one element, lest it crashes\n",
        "  if len(roi_bounds) <= 0:\n",
        "    return [0], True\n",
        "\n",
        "  # Sort components into two columns using KMeans\n",
        "  roi_bounds = sort_into_columns(roi_bounds)\n",
        "\n",
        "  # At this point, components in each column typically have one or more components in the same y (y is within Y_EPSILON)\n",
        "  # Remove components except the ones rightmost in each row\n",
        "  roi_bounds = filter_non_letters(roi_bounds)\n",
        "\n",
        "  # Dilate roi bounds to includ a little bit of the outside\n",
        "  roi_bounds = dilate_component_boxes(roi_bounds, 5)\n",
        "\n",
        "  # Handle exception: If the number of ROIs found is not the same as the target, consider the image invalid\n",
        "  if len(roi_bounds) != num_of_items:\n",
        "    return [0], True\n",
        "\n",
        "  return roi_bounds, False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TrffyMX1XYB"
      },
      "outputs": [],
      "source": [
        "# Define helper function for ROI extraction that returns nearby pairs of centroids given a distance threshold\n",
        "# Takes in an array of centroids (component midpoints) and max merge distance\n",
        "# Returns an array of tuples, representing nearby pairs of components\n",
        "def find_nearby_pairs(centroids_array, max_merge_distance):\n",
        "  nearby_pairs = []\n",
        "  num_components = len(centroids_array)\n",
        "  for i in range(num_components - 1):\n",
        "    for j in range(i + 1, num_components):\n",
        "      distance = np.linalg.norm(np.array(centroids_array[i]) - np.array(centroids_array[j]))\n",
        "      if distance <= max_merge_distance:\n",
        "        nearby_pairs.append((i, j))\n",
        "  return nearby_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2Ludp0s1f0A"
      },
      "outputs": [],
      "source": [
        "# Define a helper function for ROI extraction that generates groups of nearby nearby-pairs\n",
        "# Takes in an array of nearby pairs\n",
        "# Returns an array of sets, representing two or more components that are close to each other\n",
        "def find_mergeable_components(nearby_pairs):\n",
        "  groups = []\n",
        "  for pair in nearby_pairs:\n",
        "    group_found = False\n",
        "    for group in groups:\n",
        "      if any(component in group for component in pair): # Evaluates, for each pair component, if it is in the group\n",
        "        group.update(pair)\n",
        "        group_found = True\n",
        "        break\n",
        "    if not group_found:\n",
        "      groups.append(set(pair))\n",
        "  return groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUKF7Ns81hPd"
      },
      "outputs": [],
      "source": [
        "# Define a helper function for ROI extraction that merges mergeable components\n",
        "# Takes in an array of mergeable groups and all the component bounds\n",
        "# Returns an array of the bounds that were the result of merging the mergeable groups\n",
        "def merge_groups(groups, bounds):\n",
        "  merged_bounds = []\n",
        "  for group in groups:\n",
        "    min_x = 5000\n",
        "    min_y = 5000\n",
        "    max_x = 0\n",
        "    max_y = 0\n",
        "    for component in group:\n",
        "      x, y, w, h = bounds[component]\n",
        "      min_x = min(min_x, x)\n",
        "      min_y = min(min_y, y)\n",
        "      max_x = max(max_x, x+w)\n",
        "      max_y = max(max_y, y+h)\n",
        "    merged_bounds.append([min_x, min_y, max_x - min_x, max_y - min_y])\n",
        "  return merged_bounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92pqbVg_1iYp"
      },
      "outputs": [],
      "source": [
        "# Define a helper function for ROI extraction that sorts a list of bounds into k columns using KMeans clustering\n",
        "# Takes in an array of component bounds\n",
        "# Returns an array of component bounds that are sorted into k columns\n",
        "def sort_into_columns(component_bounds):\n",
        "  # Determine the x coordinates of the bounds\n",
        "  x_coordinates = [bound[0] for bound in component_bounds]\n",
        "  x_coordinates = np.array(x_coordinates).reshape(-1, 1)\n",
        "\n",
        "  # Set the optimal number of clusters (k) to NUMBER_OF_COLUMNS, as the number of columns is predefined\n",
        "  optimal_k = NUMBER_OF_COLUMNS\n",
        "\n",
        "  # Perform K-means clustering with k = NUMBER_OF_COLUMNS\n",
        "  kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "  kmeans.fit(x_coordinates)\n",
        "\n",
        "  # Group the components based on the cluster assignments\n",
        "  grouped_components = [[] for _ in range(optimal_k)] # An array of two arrays\n",
        "  for i, label in enumerate(kmeans.labels_): # kmeans.labels_ is an array of ints representing the label of each component\n",
        "    grouped_components[label].append(component_bounds[i])\n",
        "\n",
        "  # Sort into a single list\n",
        "  sorted_components = []\n",
        "  grouped_components = sorted(grouped_components, key=lambda group: group[0][0])\n",
        "  for group in grouped_components:\n",
        "    sorted_group = sorted(group, key=lambda component: component[1])\n",
        "    sorted_components.extend(sorted_group)\n",
        "\n",
        "  return sorted_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMA60xDa1jWt"
      },
      "outputs": [],
      "source": [
        "# Define a helper function for ROI extraction that removes non-letter components' bounds\n",
        "# Takes in an array of component bounds\n",
        "# Returns an array of component bounds that excludes positionally considered to be non-letters\n",
        "def filter_non_letters(component_bounds):\n",
        "  # Define dictionaries mapping components to their origin-x and centroid-y\n",
        "  comp_x_dict = {}\n",
        "  for index, bound in enumerate(component_bounds):\n",
        "    comp_x_dict[index] = bound[0]\n",
        "\n",
        "  comp_cent_y_dict = {}\n",
        "  for index, bound in enumerate(component_bounds):\n",
        "    comp_cent_y_dict[index] = bound[1] + (bound[3] / 2)\n",
        "\n",
        "  # --------------------------------------------------------------------------------------------------------------------\n",
        "  # Define a function that compares two keys and decides if the current key should be removed\n",
        "  # Takes in the current key, the key to check against, a dictionary of component origin-x, and a dicitonary of component centroid-y\n",
        "  # Modifies the array directly; returns True if the current key has been removed\n",
        "  def check_key_for_removal(curr_key, key_to_check, x_dict, y_dict):\n",
        "    if not key_to_check in y_dict or not curr_key in y_dict:\n",
        "      return False\n",
        "    if abs(y_dict[key_to_check] - y_dict[curr_key]) > Y_EPSILON:\n",
        "      return False\n",
        "\n",
        "    curr_key_x = x_dict[curr_key]\n",
        "    key_to_check_x = x_dict[key_to_check]\n",
        "    if key_to_check_x > curr_key_x:\n",
        "      y_dict.pop(curr_key)\n",
        "      return True\n",
        "  # --------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Based on the components centroid-y, determine which components are in the same \"row,\" i.e. components whose centroid-y are within Y_EPSILON\n",
        "  # If the components have significantly different centroid-y, ignore; else, check if the current one is to the left or to the right relatively\n",
        "  # If the current component is to the left of some other component in the same \"row,\" remove it from the comp_cent_y_dict\n",
        "  dup_y_dict = comp_cent_y_dict.copy()\n",
        "  for key in dup_y_dict:\n",
        "    prev_prev_key = key - 2\n",
        "    prev_key = key - 1\n",
        "    next_key = key + 1\n",
        "    if not key in comp_cent_y_dict:\n",
        "      continue\n",
        "\n",
        "    done = check_key_for_removal(key, prev_prev_key, comp_x_dict, comp_cent_y_dict)\n",
        "    if done: continue # If the curr_key has been removed, go to the next key\n",
        "\n",
        "    done = check_key_for_removal(key, prev_key, comp_x_dict, comp_cent_y_dict)\n",
        "    if done: continue\n",
        "\n",
        "    done = check_key_for_removal(key, next_key, comp_x_dict, comp_cent_y_dict)\n",
        "\n",
        "  # Create an array of component bounds that only excludes what remains in the comp_cent_y_dict\n",
        "  filtered_component_bounds = [bound for index, bound in enumerate(component_bounds) if index in comp_cent_y_dict]\n",
        "\n",
        "  return filtered_component_bounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcEHmeHjWvR4"
      },
      "outputs": [],
      "source": [
        "# Define a helper function for ROI extraction that dilates component bounds (increases bounds)\n",
        "# Takes in an array of component bounds and an optional dilation size\n",
        "# Returns an array of new component bounds that are dilated if possible\n",
        "def dilate_component_boxes(component_bounds, dilation_size=1):\n",
        "  new_components = []\n",
        "  for bound in component_bounds:\n",
        "    new_bound = [bound[0] - 1 * dilation_size, bound[1] - 1 * dilation_size, bound[2] + 2 * dilation_size, bound[3] + 2 * dilation_size]\n",
        "    x, y, w, h = new_bound\n",
        "\n",
        "    # Check in case out of image size\n",
        "    if (x < 0 or x > IMAGE_WIDTH) or (y < 0 or y > IMAGE_HEIGHT) or (x + w > IMAGE_WIDTH) or (y + h > IMAGE_HEIGHT):\n",
        "      new_components.append(bound)\n",
        "      continue\n",
        "\n",
        "    new_components.append(new_bound)\n",
        "  return new_components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeW50SezRA9U"
      },
      "source": [
        "### Image Classification Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S0V_bbVkknf"
      },
      "outputs": [],
      "source": [
        "# Define the classify ROIs function, that inputs the ROIs into the image classifier after a few processing steps\n",
        "# Takes in an array of ROI bounds and the ocr image to get the images from\n",
        "# Returns an array of item answers (letter string)\n",
        "def classify_rois(roi_bounds, ocr_image):\n",
        "  item_answers = []\n",
        "  for roi_bound in roi_bounds:\n",
        "    x, y, w, h = roi_bound\n",
        "    roi = ocr_image[y:y+h, x:x+w]\n",
        "    item_answers.append(classify_roi(roi))\n",
        "  return item_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWsrH1MTRBnX"
      },
      "outputs": [],
      "source": [
        "# Define a helper function for ROI classification for readability\n",
        "# Takes in an ROI expressed as a numpy array of shape [y, x]\n",
        "# Returns the classifier's classification of the ROI\n",
        "def classify_roi(roi):\n",
        "  # Preprocess ROI\n",
        "  new_array = cv2.resize(roi, (28, 28))\n",
        "\n",
        "  roi = new_array\n",
        "  roi = cv2.bitwise_not(roi) # Invert to fit model requirement\n",
        "  roi = roi / 255.0 # Normalize\n",
        "  roi = np.where(roi > 0, 1, 0)\n",
        "\n",
        "  roi = torch.from_numpy(roi) # Convert to tensor\n",
        "  roi = roi.view(1, 1, 28, 28) # Reshape to fit model requirement\n",
        "  roi = roi.to(torch.float32) # Change data type to fit model requirement\n",
        "\n",
        "  # Classify the ROI\n",
        "  model.eval()\n",
        "  output = model(roi) # Output is a tensor with 4 floats representing class probabilities\n",
        "  prediction = output.argmax(dim=1, keepdim=True).item() # Returns a number from 0 to 3, representing the most probable class\n",
        "  predicted_letter = PREDICTION_TO_STRING[prediction] # Converts the number to the corresponding letter\n",
        "\n",
        "  return predicted_letter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CTZ7GAUikON"
      },
      "source": [
        "### Grading and Analysis Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxkhdVWriaz9"
      },
      "outputs": [],
      "source": [
        "# Define the grading and analysis function, that updates result arrays and returns an annotated corrected image (numpy matrix)\n",
        "# Takes in 5 result arrays, one string of text, the ocr image, and the array of ROI bounds\n",
        "# Returns the corrected image and updates the arrays in place\n",
        "def grade_and_analyze(item_answers, img_cor_arr, img_scr_arr, itm_ana_arr, scr_ana_arr, correction_key, ocr_image, roi_bounds):\n",
        "  corrected_image = np.copy(ocr_image)\n",
        "  corrected_image = cv2.cvtColor(corrected_image, cv2.COLOR_GRAY2BGR)\n",
        "  correction_array = []\n",
        "  score = 0\n",
        "\n",
        "  for i in range(len(correction_key)):\n",
        "\n",
        "    if item_answers[i] == correction_key[i] or correction_key[i] == \"X\":\n",
        "      correction_array.append(1) # Update correction array\n",
        "      score += 1 # Update score\n",
        "      itm_ana_arr[i] += 1 # Update item analysis\n",
        "    else:\n",
        "      correction_array.append(0) # Update correction array\n",
        "      # Add correction to image\n",
        "      x, y, w, h = roi_bounds[i]\n",
        "      cv2.putText(corrected_image, correction_key[i], (x + w + 10, y + h), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "  # Write a score on the upper right\n",
        "  cv2.putText(corrected_image, f\"{score}/{len(correction_key)}\", (IMAGE_WIDTH - 250, 200), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)\n",
        "\n",
        "  # Bring changes to the input arrays (except item analysis, which was updated during the loop)\n",
        "  img_cor_arr.append(correction_array)\n",
        "  img_scr_arr.append(score)\n",
        "  scr_ana_arr[score] += 1 # Update score analysis\n",
        "  return corrected_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aolbZpscRhXM"
      },
      "source": [
        "## THE INTERFACE\n",
        "Use Gradio to create a user-friendly interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_LR6TyoZxVJ"
      },
      "outputs": [],
      "source": [
        "# Download the sample images for the demo interface\n",
        "SAMPLE_IMAGES_FOLDER_URL = \"https://raw.githubusercontent.com/GabrielEdradan/HMC-Grad/main/sample_images/\"\n",
        "IMAGE_SAVE_PATH = \"/content/\"\n",
        "\n",
        "NUM_OF_SAMPLES = 2\n",
        "for i in range(1, NUM_OF_SAMPLES + 1):\n",
        "  suffix = f\"sample_{i}.jpg\"\n",
        "  os.system(f\"wget {SAMPLE_IMAGES_FOLDER_URL}{suffix} -O {IMAGE_SAVE_PATH}{suffix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnBH_XUARhyV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define the interface function\n",
        "# Takes in two inputs: the correction key (string) and the answer sheet images (Array[Array[image paths]])\n",
        "# Returns the results of the process_image_set function\n",
        "def interface_function(correction_key_string, *images):\n",
        "\n",
        "  # Set correction key\n",
        "  correction_key = []\n",
        "  for item in correction_key_string:\n",
        "    if item in [\"A\", \"B\", \"C\", \"D\", \"X\"]:\n",
        "      correction_key.append(item.upper())\n",
        "\n",
        "  # Run the main function\n",
        "  return process_image_set(images[0], correction_key)\n",
        "\n",
        "\n",
        "# Set the description string\n",
        "desc ='''\n",
        "This is the demo interface for the research project titled \"HMC-Grad: Automating Handwritten Multiple-Choice Test Grading Using Computer Vision and Deep Learning\" by Edradan, G., Serrano, D., and Tunguia, T.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "First Input: Enter the correction key. It must be a continuous string of text, with each letter representing the correct answer for each item in consecutive order. The only letters accepted are A, B, C, and D, but an X can be written to represent an item that would accept any answer (bonus item).\n",
        "\n",
        "Second Input: Upload the images of the papers to be evaluated and analyzed. The order of evaluation is based on the order in which the images are uploaded.\n",
        "\n",
        "For better results, the following are recommended:\n",
        "\n",
        "Regarding the documents to be evaluated:\n",
        "  - Have substantial left and right margins\n",
        "  - Provide a blank line between the header and the answers\n",
        "  - Write the answers in capitals and in two columns\n",
        "  - Avoid having the answers overlap with the notepad lines\n",
        "  - Have significant space between the numbers and the letters\n",
        "  - Write the item numbers smaller than the letters\n",
        "\n",
        "Regarding the photo:\n",
        "  - Have an aspect ratio of 3:4\n",
        "  - Have a resolution of at least 1125 px by 1500 px\n",
        "  - Have adequate lighting; use flash if necessary\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# Define the Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=interface_function,\n",
        "    title=\"HMC-Grad: Handwritten Multiple-Choice Test Grader\",\n",
        "    description=desc,\n",
        "    allow_flagging=\"never\",\n",
        "    inputs=[gr.Textbox(label=\"Correction Key\", placeholder=\"ABCDABCDABCD\"),\n",
        "            gr.File(file_count=\"multiple\", file_types=[\".jpg\"], label=\"Upload Image(s)\")],\n",
        "    outputs=[gr.File(label=\"Correctness\"),\n",
        "             gr.File(label=\"Scores\"),\n",
        "             gr.File(label=\"Item Analysis\"),\n",
        "             gr.File(label=\"Score Analysis\"),\n",
        "             gr.File(label=\"Merged Data\"),\n",
        "             gr.File(label=\"Corrected Images\"),\n",
        "             ],\n",
        "    examples=[\n",
        "        [\"ABCDXABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDA\",(['sample_1.jpg'])],\n",
        "        [\"ABCDXABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDA\",(['sample_2.jpg'])],\n",
        "        [\"ABCDXABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDA\",(['sample_1.jpg', 'sample_2.jpg'])],\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch(debug=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z2oy94RRxJa"
      },
      "source": [
        "# HMC-GRAD: HANDWRITTEN MULTIPLE-CHOICE TEST GRADER\n",
        "The implementation of HMC-Grad, employing OpenCV for image preprocessing and PyTorch for training a convolutional neural network on the EMNIST dataset for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaklWdMZRwYU"
      },
      "source": [
        "## IMPORT MODULES AND SET GLOBALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5Zz7NinfDDM",
        "outputId": "f6555720-b483-4cf8-8d95-0ab530d02974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.14.0-py3-none-any.whl (16.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.8.0 (from gradio)\n",
            "  Downloading gradio_client-0.8.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.1/305.1 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Collecting pydantic>=2.0 (from gradio)\n",
            "  Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.8.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.8.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.6 (from pydantic>=2.0->gradio)\n",
            "  Downloading pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=4.0 (from gradio)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.36.0,>=0.35.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx->gradio)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=6bf054888300bb0e901364bd78e5f6024f5fafc6636ec3a1c973b9b522ecd753\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, typing-extensions, tomlkit, shellingham, semantic-version, python-multipart, orjson, h11, colorama, annotated-types, aiofiles, uvicorn, starlette, pydantic-core, httpcore, pydantic, httpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 annotated-types-0.6.0 colorama-0.4.6 fastapi-0.109.0 ffmpy-0.3.1 gradio-4.14.0 gradio-client-0.8.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 orjson-3.9.10 pydantic-2.5.3 pydantic-core-2.14.6 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.35.1 tomlkit-0.12.0 typing-extensions-4.9.0 uvicorn-0.25.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_4U_dUaQs2i-"
      },
      "outputs": [],
      "source": [
        "# For downloading model weights\n",
        "import os\n",
        "\n",
        "# For image preprocessing\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# For the image classifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# For the interface\n",
        "import gradio as gr\n",
        "\n",
        "# For formatting data\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RrOzL7g0zab_"
      },
      "outputs": [],
      "source": [
        "# Globals Constants:\n",
        "\n",
        "# For preprocessing\n",
        "IMAGE_WIDTH = 1125\n",
        "IMAGE_HEIGHT = 1500\n",
        "THRESH_BLOCK_SIZE = 11\n",
        "THRESH_CONSTANT = 5\n",
        "LINE_LENGTH = 5000\n",
        "\n",
        "# For ROI extraction\n",
        "MAX_COMPONENT_MERGE_DISTANCE = 30\n",
        "MIN_COMPONENT_SIDE = 15\n",
        "Y_EPSILON = 25\n",
        "NUMBER_OF_COLUMNS = 2\n",
        "\n",
        "# For image classification\n",
        "PREDICTION_TO_STRING = [\"A\", \"B\", \"C\", \"D\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFv3JBjAOy08"
      },
      "source": [
        "## THE CLASSIFIER\n",
        "Prepare the Classifier Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yK9WDGQQoKOA"
      },
      "outputs": [],
      "source": [
        "# Download the model's state dictionary from repository\n",
        "GITHUB_URL = \"https://raw.githubusercontent.com/GabrielEdradan/HMC-Grad/main/image_classifier.pth\"\n",
        "MODEL_PATH = \"/content/model.pth\"\n",
        "os.system(f\"wget {GITHUB_URL} -O {MODEL_PATH}\")\n",
        "model_state_dict = torch.load(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "avmc5ro9s1s1"
      },
      "outputs": [],
      "source": [
        "# Define the model class\n",
        "class ABCDClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ABCDClassifier, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(32 * 4 * 4, 128)\n",
        "    self.fc2 = nn.Linear(128, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 32 * 4 * 4)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return F.softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3XUKncaOrDJ",
        "outputId": "5b7499b4-fc73-4cf5-f3eb-8b955cf51a96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Load the model\n",
        "model = ABCDClassifier()\n",
        "model.load_state_dict(model_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsWpajjjOtmT"
      },
      "source": [
        "## THE IMAGE PROCESSOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q5HWgmSQx4t"
      },
      "source": [
        "### The Image Processor Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UaVWpmkRtXFN"
      },
      "outputs": [],
      "source": [
        "# The main function used for the interface.\n",
        "# Takes in an array of strings (image paths), and an array of chars (correction key)\n",
        "# Returns four str file paths to csv files: correctness, scores, item analysis, and score analysis\n",
        "def process_image_set(image_paths_array, correction_key):\n",
        "  correctness = [] # an array of binary int arrays, of length len(image_paths_array)\n",
        "  scores = [] # an array of ints, of length len(image_paths_array)\n",
        "  item_analysis = [0] * len(correction_key) # an array of ints\n",
        "  score_analysis = [0] * (len(correction_key) + 1) # an array of ints\n",
        "\n",
        "  for i in range(len(image_paths_array)):\n",
        "    process_image(image_paths_array[i], correctness, scores, item_analysis, score_analysis, correction_key)\n",
        "\n",
        "  # Formatting data\n",
        "  # Define csv file paths\n",
        "  student_correctness_csv_path = \"student_correctness.csv\"\n",
        "  student_scores_csv_path = \"student_scores.csv\"\n",
        "  item_analysis_csv_path = \"item_analysis.csv\"\n",
        "  score_analysis_csv_path = \"score_analysis.csv\"\n",
        "  merged_xlsx_path = \"merged_data.xlsx\"\n",
        "\n",
        "  # For correctness\n",
        "  transposed_data = list(map(list, zip(*correctness))) # Transpose the data to have students as columns\n",
        "  columns = [f\"Student {i+1}\" for i in range(len(correctness))] # Define the columns\n",
        "  correctness_df = pd.DataFrame(transposed_data, columns=columns) # Create the DataFrame\n",
        "  correctness_df.insert(0, \"Item Number\", range(1, len(correctness[0]) + 1)) # Add the item number column\n",
        "  correctness_df.to_csv(student_correctness_csv_path, index=False) # Save\n",
        "\n",
        "  # For student scores\n",
        "  columns = [\"Score\"] # Define the columns\n",
        "  scores_df = pd.DataFrame(scores, columns=columns) # Create the DataFrame\n",
        "  scores_df.insert(0, \"Student Number\", range(1, len(scores) + 1)) # Add the student number column\n",
        "  scores_df.to_csv(student_scores_csv_path, index=False) # Save\n",
        "\n",
        "  # For item analysis\n",
        "  columns = [\"Number of Correct Answers\"] # Define the columns\n",
        "  item_analysis_df = pd.DataFrame(item_analysis, columns=columns) # Create the DataFrame\n",
        "  item_analysis_df.insert(0, \"Item Number\", range(1, len(item_analysis) + 1)) # Add the student number column\n",
        "  item_analysis_df.to_csv(item_analysis_csv_path, index=False) # Save\n",
        "\n",
        "  # For score analysis\n",
        "  columns = [\"Number of Students\"] # Define the columns\n",
        "  score_analysis_df = pd.DataFrame(score_analysis, columns=columns) # Create the DataFrame\n",
        "  score_analysis_df.insert(0, \"Score\", range(0, len(score_analysis))) # Add the student number column\n",
        "  score_analysis_df.to_csv(score_analysis_csv_path, index=False) # Save\n",
        "\n",
        "  # For merging CSV into into XLSX\n",
        "  # Create a writer to save multiple dataframes to a single XLSX file\n",
        "  with pd.ExcelWriter(merged_xlsx_path) as writer:\n",
        "      # Write each dataframe to a different sheet\n",
        "      correctness_df.to_excel(writer, sheet_name=\"Correctness\", index=False)\n",
        "      scores_df.to_excel(writer, sheet_name=\"Scores\", index=False)\n",
        "      item_analysis_df.to_excel(writer, sheet_name=\"Item Analysis\", index=False)\n",
        "      score_analysis_df.to_excel(writer, sheet_name=\"Score Analysis\", index=False)\n",
        "\n",
        "  return student_correctness_csv_path, student_scores_csv_path, item_analysis_csv_path, score_analysis_csv_path, merged_xlsx_path\n",
        "\n",
        "\n",
        "# A helper function for readability\n",
        "# Takes in an image path, the four arrays to be modified and an array of chars\n",
        "# Void return value, modifies the four arrays directly\n",
        "def process_image(img_path, img_cor_arr, img_scr_arr, itm_ana_arr, scr_ana_arr, correction_key):\n",
        "  base_image = cv2.imread(img_path)\n",
        "\n",
        "  # Preprocessing\n",
        "  segmentation_image, ocr_image, processing_error = preprocess(base_image)\n",
        "  if processing_error: # Check for exception\n",
        "    invalid_num_of_items(img_scr_arr, \"PROCESSING ERROR\")\n",
        "    return\n",
        "\n",
        "  # ROI Extraction\n",
        "  rois, extraction_error = extract_rois(segmentation_image, ocr_image, len(correction_key))\n",
        "  if extraction_error: # Check for exception\n",
        "    invalid_num_of_items(img_scr_arr, \"EXTRACTION ERROR\")\n",
        "    return\n",
        "\n",
        "  # Classification\n",
        "  item_answers = classify_rois(rois)\n",
        "\n",
        "  if len(item_answers) != len(correction_key): # Check for exception (extra layer of safety)\n",
        "    invalid_num_of_items(img_scr_arr, \"NUM OF ITEM ANSWERS != CORRECTION KEY\")\n",
        "    return\n",
        "\n",
        "  # Grading and Analysis\n",
        "  grade_and_analyze(item_answers, img_cor_arr, img_scr_arr, itm_ana_arr, scr_ana_arr, correction_key)\n",
        "\n",
        "\n",
        "# Function for error logic: sets the score to -1 (-1 score means invalid image)\n",
        "# Takes in the array to be modified (score array)\n",
        "# Void return value, modifies the array directly\n",
        "def invalid_num_of_items(score_array, error_string):\n",
        "  score_array.append(-1)\n",
        "  print(f\"ERROR: {error_string}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwhaA5O4QsDv"
      },
      "source": [
        "### Image Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JzWGfdPG58Cb"
      },
      "outputs": [],
      "source": [
        "# Takes in an image (the base image)\n",
        "# Returns an image for segmentation, an image for OCR, and a boolean for error handling\n",
        "def preprocess(input_image):\n",
        "  # ------------------------------------ FOR OCR IMAGE------------------------------------ #\n",
        "  resized_image = cv2.resize(input_image, (IMAGE_WIDTH, IMAGE_HEIGHT)) # MOVED FROM INTERFACE TO HERE\n",
        "  gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "  threshed_gray = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, THRESH_BLOCK_SIZE, THRESH_CONSTANT)\n",
        "  opened = cv2.bitwise_not(cv2.morphologyEx(cv2.bitwise_not(threshed_gray), cv2.MORPH_OPEN, np.ones((2,2),np.uint8), iterations=1))\n",
        "  ocr_image = remove_lines(opened)\n",
        "\n",
        "  # ----------------------------------- FOR HEADER MASK ----------------------------------- #\n",
        "  blur = cv2.GaussianBlur(gray, (9,9), 0)\n",
        "  threshed_blur = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, THRESH_BLOCK_SIZE, THRESH_CONSTANT)\n",
        "  removed_lines = remove_lines(threshed_blur, 6)\n",
        "\n",
        "  # Determine divider y positions\n",
        "  intensely_dilated = cv2.dilate(cv2.bitwise_not(removed_lines), cv2.getStructuringElement(cv2.MORPH_RECT, (2000, 40)), iterations=1)\n",
        "  black_ys = np.where(np.any(intensely_dilated == 0, axis=1))[0].tolist()\n",
        "  to_be_removed = []\n",
        "  for i in range(1, len(black_ys)):\n",
        "    if black_ys[i] - black_ys[i - 1] == 1:\n",
        "      to_be_removed.append(i)\n",
        "  stripe_positions = [black_ys[idx] for idx in range(len(black_ys)) if not idx in to_be_removed]\n",
        "\n",
        "  # Determine which stripe is the header border\n",
        "  num_of_stripe_positions = len(stripe_positions)\n",
        "  header_border = 0\n",
        "  match num_of_stripe_positions:\n",
        "    case 1:\n",
        "      header_border = stripe_positions[0]\n",
        "    case 2:\n",
        "      header_border = stripe_positions[0] if stripe_positions[1] > 500 else stripe_positions[1]\n",
        "    case _:\n",
        "      found = False\n",
        "      for stripe_position in stripe_positions:\n",
        "        if stripe_position > 100 and stripe_position < 500:\n",
        "          header_border = stripe_position\n",
        "          found = True\n",
        "          continue\n",
        "      if not found: # If there is no stripe, consider the image invalid (subject to change)\n",
        "        return [0], [0], True\n",
        "\n",
        "  # Create a mask based on header border\n",
        "  mask = np.ones(intensely_dilated.shape, dtype=np.uint8) * 255\n",
        "  mask[header_border:, :] = 0\n",
        "\n",
        "  # ---------------------------------- FOR COMPONENT FINDING IMAGE ---------------------------------- #\n",
        "  masked_removed_lines = cv2.bitwise_or(removed_lines, mask)\n",
        "  segmentation_image = cv2.dilate(cv2.bitwise_not(masked_removed_lines), cv2.getStructuringElement(cv2.MORPH_RECT, (10, 6)), iterations=1)\n",
        "\n",
        "  return segmentation_image, ocr_image, False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "oxn1iQUX7be6"
      },
      "outputs": [],
      "source": [
        "# A helper function for the preprocess function\n",
        "# Takes in an image, and optionally line thickness (an int that determines how erased the notepad lines are)\n",
        "# Returns a modified image, where the notepad lines are attempted to be erased\n",
        "def remove_lines(input_image, thickness=2):\n",
        "  edges = cv2.Canny(input_image, 200, 200)\n",
        "  lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)\n",
        "\n",
        "  # Check for when lines are not found\n",
        "  if len(lines) == 0:\n",
        "    return input_image\n",
        "\n",
        "  # Creates a mask based on the lines found by HoughLines\n",
        "  line_mask = np.zeros_like(input_image)\n",
        "  for line in lines:\n",
        "    rho, theta = line[0]\n",
        "    if theta < 1.0 or theta > 2.0:\n",
        "      continue\n",
        "    a = np.cos(theta)\n",
        "    b = np.sin(theta)\n",
        "    x0 = a * rho\n",
        "    y0 = b * rho\n",
        "    x1 = int(x0 + LINE_LENGTH * (-b))\n",
        "    y1 = int(y0 + LINE_LENGTH * (a))\n",
        "    x2 = int(x0 - LINE_LENGTH * (-b))\n",
        "    y2 = int(y0 - LINE_LENGTH * (a))\n",
        "    cv2.line(line_mask, (x1, y1), (x2, y2), (255, 255, 255), 2)\n",
        "\n",
        "  # Dilates the lines vertically based on thickness parameter\n",
        "  dilation_kernel = np.ones((thickness, 1),np.uint8)\n",
        "  line_mask = cv2.dilate(line_mask, dilation_kernel, iterations=1)\n",
        "\n",
        "  # Subtracts the mask from the base image and applies MORPH_OPEN to denoise\n",
        "  sub_result = cv2.bitwise_or(input_image, line_mask)\n",
        "  final_result = cv2.bitwise_not(cv2.morphologyEx(cv2.bitwise_not(sub_result), cv2.MORPH_OPEN, np.ones((2, 2),np.uint8), iterations=1))\n",
        "  return final_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61Yx4uiBQ4K0"
      },
      "source": [
        "### Region of Interest Extraction Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mgIH1Imp1EQ4"
      },
      "outputs": [],
      "source": [
        "# Takes in an image for segmentation, an image for OCR, and the target number of ROIs\n",
        "# Returns an array of RIOs (of shape [y, x]), and a boolean for error handling\n",
        "def extract_rois(segmentation_image, ocr_image, num_of_items):\n",
        "  # Get the components in the input image\n",
        "  _, _, stats, centroids = cv2.connectedComponentsWithStats(segmentation_image, connectivity=4)\n",
        "\n",
        "  # Define all bounds (stats excluding area)\n",
        "  all_bounds = [stat[:4].tolist() for stat in stats]\n",
        "\n",
        "  # Remove the background from bounds and centroids\n",
        "  all_bounds.pop(0)\n",
        "  centroids = centroids.tolist()\n",
        "  centroids.pop(0)\n",
        "\n",
        "  # Find components that are close to each other and merge them\n",
        "  nearby_component_pairs = find_nearby_pairs(centroids, MAX_COMPONENT_MERGE_DISTANCE)\n",
        "  mergeable_components = find_mergeable_components(nearby_component_pairs)\n",
        "  merged_bounds = merge_groups(mergeable_components, all_bounds)\n",
        "\n",
        "  # Make an array of bounds, exclude bounds that were used in merging and bounds that are too small\n",
        "  component_bounds = [bound[:4] for index, bound in enumerate(all_bounds) if not any(index in group for group in mergeable_components) and (bound[2] > MIN_COMPONENT_SIDE and bound[3] > MIN_COMPONENT_SIDE)]\n",
        "\n",
        "  # Add the merged bounds\n",
        "  component_bounds.extend(merged_bounds)\n",
        "\n",
        "  # Sort components into two columns\n",
        "  component_bounds = sort_into_columns(component_bounds)\n",
        "\n",
        "  # At this point, components in each column typically have one or more components in the same y (y is within Y_EPSILON)\n",
        "  # Remove components except the ones rightmost in each row\n",
        "  component_bounds = filter_non_letters(component_bounds)\n",
        "\n",
        "  # Convert bounds to ROIs\n",
        "  rois = []\n",
        "  for bound in component_bounds:\n",
        "    x, y, w, h = bound\n",
        "    roi_img = ocr_image[y:y+h, x:x+w]\n",
        "    rois.append(roi_img)\n",
        "\n",
        "  # Handle exception: If the number of ROIs found is not the same as the target, consider the image invalid\n",
        "  if len(rois) != num_of_items:\n",
        "    return [0], True\n",
        "\n",
        "  return rois, False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1TrffyMX1XYB"
      },
      "outputs": [],
      "source": [
        "# A helper function for the roi extraction function\n",
        "# Takes in an array of centroids (component midpoints) and max merge distance\n",
        "# Returns an array of tuples, representing nearby pairs of components\n",
        "def find_nearby_pairs(centroids_array, max_merge_distance):\n",
        "  nearby_pairs = []\n",
        "  num_components = len(centroids_array)\n",
        "  for i in range(num_components - 1):\n",
        "    for j in range(i + 1, num_components):\n",
        "      distance = np.linalg.norm(np.array(centroids_array[i]) - np.array(centroids_array[j]))\n",
        "      if distance <= max_merge_distance:\n",
        "        nearby_pairs.append((i, j))\n",
        "  return nearby_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Y2Ludp0s1f0A"
      },
      "outputs": [],
      "source": [
        "# A helper function for the roi extraction function\n",
        "# Takes in an array of nearby pairs\n",
        "# Returns an array of sets, representing two or more components that are close to each other\n",
        "def find_mergeable_components(nearby_pairs):\n",
        "  groups = []\n",
        "  for pair in nearby_pairs:\n",
        "    group_found = False\n",
        "    for group in groups:\n",
        "      if any(component in group for component in pair): # Evaluates, for each pair component, if it is in the group\n",
        "        group.update(pair)\n",
        "        group_found = True\n",
        "        break\n",
        "    if not group_found:\n",
        "      groups.append(set(pair))\n",
        "  return groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "iUKF7Ns81hPd"
      },
      "outputs": [],
      "source": [
        "# A helper function for the roi extraction function\n",
        "# Takes in an array of mergeable groups and all the component bounds\n",
        "# Returns an array of the bounds that were the result of merging the mergeable groups\n",
        "def merge_groups(groups, bounds):\n",
        "  merged_bounds = []\n",
        "  for group in groups:\n",
        "    min_x = 5000\n",
        "    min_y = 5000\n",
        "    max_x = 0\n",
        "    max_y = 0\n",
        "    for component in group:\n",
        "      x, y, w, h = bounds[component]\n",
        "      min_x = min(min_x, x)\n",
        "      min_y = min(min_y, y)\n",
        "      max_x = max(max_x, x+w)\n",
        "      max_y = max(max_y, y+h)\n",
        "    merged_bounds.append([min_x, min_y, max_x - min_x, max_y - min_y])\n",
        "  return merged_bounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "92pqbVg_1iYp"
      },
      "outputs": [],
      "source": [
        "# A helper function for the roi extraction function\n",
        "# Takes in an array of component bounds\n",
        "# Returns an array of component bounds that are sorted into k columns\n",
        "def sort_into_columns(component_bounds):\n",
        "  # Determine the x coordinates of the bounds\n",
        "  x_coordinates = [bound[0] for bound in component_bounds]\n",
        "  x_coordinates = np.array(x_coordinates).reshape(-1, 1)\n",
        "\n",
        "  # Set the optimal number of clusters (k) to NUMBER_OF_COLUMNS, as the number of columns is predefined\n",
        "  optimal_k = NUMBER_OF_COLUMNS\n",
        "\n",
        "  # Perform K-means clustering with k = NUMBER_OF_COLUMNS\n",
        "  kmeans = KMeans(n_clusters=optimal_k, init=\"k-means++\", max_iter=300, n_init=10, random_state=0)\n",
        "  kmeans.fit(x_coordinates)\n",
        "\n",
        "  # Group the components based on the cluster assignments\n",
        "  grouped_components = [[] for _ in range(optimal_k)] # An array of two arrays\n",
        "  for i, label in enumerate(kmeans.labels_): # kmeans.labels_ is an array of ints representing the label of each component\n",
        "    grouped_components[label].append(component_bounds[i])\n",
        "\n",
        "  # Sort into a single list\n",
        "  sorted_components = []\n",
        "  grouped_components = sorted(grouped_components, key=lambda group: group[0][0])\n",
        "  for group in grouped_components:\n",
        "    sorted_group = sorted(group, key=lambda component: component[1])\n",
        "    sorted_components.extend(sorted_group)\n",
        "\n",
        "  return sorted_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hMA60xDa1jWt"
      },
      "outputs": [],
      "source": [
        "# A helper function for the roi extraction function\n",
        "# Takes in an array of component bounds\n",
        "# Returns an array of component bounds that excludes positionally considered to be non-letters\n",
        "def filter_non_letters(component_bounds):\n",
        "  # Defines dictionaries mapping components to their origin-x and centroid-y\n",
        "  comp_x_dict = {}\n",
        "  for index, bound in enumerate(component_bounds):\n",
        "    comp_x_dict[index] = bound[0]\n",
        "\n",
        "  comp_cent_y_dict = {}\n",
        "  for index, bound in enumerate(component_bounds):\n",
        "    comp_cent_y_dict[index] = bound[1] + (bound[3] / 2)\n",
        "\n",
        "\n",
        "  # Function that compares two keys and decides if the current key should be removed\n",
        "  # Takes in the current key, the key to check against, a dictionary of component origin-x, and a dicitonary of component centroid-y\n",
        "  # Modifies the array directly; returns True if the current key has been removed\n",
        "  def check_key_for_removal(curr_key, key_to_check, x_dict, y_dict):\n",
        "    if not key_to_check in y_dict or not curr_key in y_dict:\n",
        "      return False\n",
        "    if abs(y_dict[key_to_check] - y_dict[curr_key]) > Y_EPSILON:\n",
        "      return False\n",
        "\n",
        "    curr_key_x = x_dict[curr_key]\n",
        "    key_to_check_x = x_dict[key_to_check]\n",
        "    if key_to_check_x > curr_key_x:\n",
        "      y_dict.pop(curr_key)\n",
        "      return True\n",
        "\n",
        "  # Based on the components centroid-y, determine which components are in the same \"row,\" i.e. components whose centroid-y are within Y_EPSILON\n",
        "  # If the components have significantly different centroid-y, ignore; else, check if the current one is to the left or to the right relatively\n",
        "  # If the current component is to the left of some other component in the same \"row,\" remove it from the comp_cent_y_dict\n",
        "  dup_y_dict = comp_cent_y_dict.copy()\n",
        "  for key in dup_y_dict:\n",
        "    prev_prev_key = key - 2\n",
        "    prev_key = key - 1\n",
        "    next_key = key + 1\n",
        "    if not key in comp_cent_y_dict:\n",
        "      continue\n",
        "\n",
        "    done = check_key_for_removal(key, prev_prev_key, comp_x_dict, comp_cent_y_dict)\n",
        "    if done: continue # If the curr_key has been removed, go to the next key\n",
        "\n",
        "    done = check_key_for_removal(key, prev_key, comp_x_dict, comp_cent_y_dict)\n",
        "    if done: continue\n",
        "\n",
        "    done = check_key_for_removal(key, next_key, comp_x_dict, comp_cent_y_dict)\n",
        "\n",
        "  # Create an array of component bounds that only excludes what remains in the comp_cent_y_dict\n",
        "  filtered_component_bounds = [bound for index, bound in enumerate(component_bounds) if index in comp_cent_y_dict]\n",
        "\n",
        "  return filtered_component_bounds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeW50SezRA9U"
      },
      "source": [
        "### Image Classification Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_S0V_bbVkknf"
      },
      "outputs": [],
      "source": [
        "# Takes in an array of ROIs (numpy arrays)\n",
        "# Returns an array of item answers (letter string)\n",
        "def classify_rois(rois):\n",
        "  item_answers = []\n",
        "  for roi in rois:\n",
        "    item_answers.append(classify_roi(roi))\n",
        "  return item_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LWsrH1MTRBnX"
      },
      "outputs": [],
      "source": [
        "# Takes in an ROI expressed as a numpy array of shape [y, x]\n",
        "# Returns the classifiers classification of the ROI\n",
        "def classify_roi(roi):\n",
        "  # Preprocess ROI\n",
        "  new_array = np.full((28, 28), 255, dtype=np.uint8)\n",
        "  small_roi = cv2.resize(roi, (26, 26))\n",
        "  new_array[1:27, 1:27] = small_roi\n",
        "\n",
        "  roi = new_array\n",
        "  roi = cv2.bitwise_not(roi) # Invert to fit model requirement\n",
        "  roi = roi / 255.0 # Normalize\n",
        "\n",
        "  roi = torch.from_numpy(roi) # Convert to tensor\n",
        "  roi = roi.view(1, 1, 28, 28) # Reshape to fit model requirement\n",
        "  roi = roi.to(torch.float32) # Change data type to fit model requirement\n",
        "\n",
        "  # Classify the ROI\n",
        "  model.eval()\n",
        "  output = model(roi) # Output is a tensor with 4 floats representing class probabilities\n",
        "  prediction = output.argmax(dim=1, keepdim=True).item() # Returns a number from 0 to 3, representing the most probable class\n",
        "  predicted_letter = PREDICTION_TO_STRING[prediction] # Converts the number to the corresponding letter\n",
        "\n",
        "  return predicted_letter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CTZ7GAUikON"
      },
      "source": [
        "### Grading and Analysis Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "RxkhdVWriaz9"
      },
      "outputs": [],
      "source": [
        "# Takes in 5 arrays and one string of text\n",
        "# Has no return value. Instead, updates the arrays in place.\n",
        "def grade_and_analyze(item_answers, img_cor_arr, img_scr_arr, itm_ana_arr, scr_ana_arr, correction_key):\n",
        "  correction_array = []\n",
        "  score = 0\n",
        "\n",
        "  for i in range(len(correction_key)):\n",
        "    if item_answers[i] == correction_key[i] or correction_key[i] == \"X\":\n",
        "      correction_array.append(1) # Update correction array\n",
        "      score += 1 # Update score\n",
        "      itm_ana_arr[i] += 1 # Update item analysis\n",
        "    else:\n",
        "      correction_array.append(0) # Update correction array\n",
        "\n",
        "  # Bring changes to the input arrays (except item analysis, which was updated during the loop)\n",
        "  img_cor_arr.append(correction_array)\n",
        "  img_scr_arr.append(score)\n",
        "  scr_ana_arr[score] += 1 # Update score analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aolbZpscRhXM"
      },
      "source": [
        "## THE INTERFACE\n",
        "Use Gradio to create a user-friendly interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1_LR6TyoZxVJ"
      },
      "outputs": [],
      "source": [
        "# Download the sample images for the demo interface\n",
        "SAMPLE_IMAGES_FOLDER_URL = \"https://raw.githubusercontent.com/GabrielEdradan/HMC-Grad/main/sample_images/\"\n",
        "IMAGE_SAVE_PATH = \"/content/\"\n",
        "\n",
        "NUM_OF_SAMPLES = 2\n",
        "for i in range(1, NUM_OF_SAMPLES + 1):\n",
        "  suffix = f\"sample_{i}.jpg\"\n",
        "  os.system(f\"wget {SAMPLE_IMAGES_FOLDER_URL}{suffix} -O {IMAGE_SAVE_PATH}{suffix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "AnBH_XUARhyV",
        "outputId": "8983bc54-1953-47a6-950a-0b5a48735d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://65d4d822e6a4e8af8d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://65d4d822e6a4e8af8d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Define the interface function\n",
        "# Takes in two inputs: the correction key (string) and the answer sheet images (Array[Array[image paths]])\n",
        "def interface_function(correction_key_string, *images):\n",
        "\n",
        "  # Set correction key\n",
        "  correction_key = []\n",
        "  for item in correction_key_string:\n",
        "    if item in [\"A\", \"B\", \"C\", \"D\", \"X\"]:\n",
        "      correction_key.append(item.upper())\n",
        "\n",
        "  # Run the main function\n",
        "  return process_image_set(images[0], correction_key)\n",
        "\n",
        "\n",
        "# Define the Gradio interface\n",
        "\n",
        "# Set the description string\n",
        "desc ='''\n",
        "This is the demo interface for the research project titled \"HMC-Grad: Automating Handwritten Multiple-Choice Test Grading Using Computer Vision and Deep Learning\" by Edradan, G., Serrano, D., and Tunguia, T.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "First Input: Enter the correction key. It must be a continuous string of text, with each letter representing the correct answer for each item in consecutive order. The only letters accepted are A, B, C, and D, but an X can be written to represent an item that would accept any answer (bonus item).\n",
        "\n",
        "Second Input: Upload the images of the papers to be evaluated and analyzed. The order of evaluation is based on the order in which the images are uploaded.\n",
        "\n",
        "For better results, the following are recommended:\n",
        "\n",
        "Regarding the documents to be evaluated:\n",
        "  - Have substantial left and right margins\n",
        "  - Provide a blank line between the header and the answers\n",
        "  - Write the answers in capitals and in two columns\n",
        "  - Avoid having the answers overlap with the notepad lines\n",
        "  - Have significant space between the numbers and the letters\n",
        "  - Write the item numbers smaller than the letters\n",
        "\n",
        "Regarding the photo:\n",
        "  - Have an aspect ratio of 3:4\n",
        "  - Have a resolution of at least 1125 px by 1500 px\n",
        "  - Have adequate lighting; use flash if necessary\n",
        "\n",
        "'''\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=interface_function,\n",
        "    title=\"HMC-Grad: Handwritten Multiple-Choice Test Grader\",\n",
        "    description=desc,\n",
        "    allow_flagging=\"never\",\n",
        "    inputs=[gr.Textbox(label=\"Correction Key\", placeholder=\"ABCDABCDABCD\"),\n",
        "            gr.File(file_count=\"multiple\", file_types=[\".jpg\"], label=\"Upload Image(s)\")],\n",
        "    outputs=[gr.File(label=\"Correctness\"),\n",
        "             gr.File(label=\"Scores\"),\n",
        "             gr.File(label=\"Item Analysis\"),\n",
        "             gr.File(label=\"Score Analysis\"),\n",
        "             gr.File(label=\"Merged Data\"),\n",
        "             ],\n",
        "    examples=[\n",
        "        [\"ABCDXABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDA\",([\"sample_1.jpg\"])],\n",
        "        [\"ABCDXABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDA\",([\"sample_2.jpg\"])],\n",
        "        [\"ABCDXABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDA\",([\"sample_1.jpg\", \"sample_2.jpg\"])],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
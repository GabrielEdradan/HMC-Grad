{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ABCD IMAGE CLASSIFIER\n",
        "A Convolutional Neural Network made with PyTorch for efficient image classification. It is trained on the EMNIST dataset on the balanced split. It takes in a 28 px by 28 px binary image and classifies it, returning a number from 0 to 3, corresponds to the letters A to D."
      ],
      "metadata": {
        "id": "4ULsQn_dj2oS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics**:\n",
        "\n",
        "Parameter Count: 79,428\n",
        "\n",
        "Average loss: 0.0076\n",
        "\n",
        "Accuracy: 98%"
      ],
      "metadata": {
        "id": "zUdlz_ErV3-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORT MODULES AND SET HYPERPARAMETERS"
      ],
      "metadata": {
        "id": "UmEP1kP1MyMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# For visualizing\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8mAnFDDY6gba"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 10\n",
        "DROPOUT_RATE = 0.5\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "INPUT_CHANNELS = 1\n",
        "OUTPUT_CLASSES = 4\n",
        "CONV1_CHANNELS = 16\n",
        "CONV2_CHANNELS = 32\n",
        "CONV_KERNEL_SIZE = 5\n",
        "MAX_POOL_KERNEL_SIZE = 2\n",
        "FCL1_CHANNELS = CONV2_CHANNELS * 4 * 4\n",
        "FCL2_CHANNELS = 128"
      ],
      "metadata": {
        "id": "N-9Q-OEZSCsc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PREPARATION"
      ],
      "metadata": {
        "id": "mtKZyP3RPRUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom transform to binarize the images\n",
        "class Binarize(object):\n",
        "    def __call__(self, img):\n",
        "        return (img > 0.5).float()"
      ],
      "metadata": {
        "id": "msxjhTTP6rWR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    lambda img: transforms.functional.rotate(img, -90),\n",
        "    lambda img: transforms.functional.hflip(img),\n",
        "    transforms.ToTensor(),\n",
        "    Binarize()\n",
        "])"
      ],
      "metadata": {
        "id": "Q9W_UwUw6voe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the EMNIST dataset\n",
        "emnist_train_dataset = datasets.EMNIST(root=\"./data\", split=\"balanced\", train=True, download=True, transform=transform)\n",
        "emnist_test_dataset = datasets.EMNIST(root=\"./data\", split=\"balanced\", train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "N2J-1NPN6wh8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels for A, B, C, and D in EMNIST\n",
        "target_labels = [10, 11, 12, 13]\n",
        "\n",
        "# Filter the dataset to get images labeled A, B, C, and D\n",
        "filtered_train_indices = [i for i in range(len(emnist_train_dataset)) if emnist_train_dataset.targets[i] in target_labels]\n",
        "train_data = torch.utils.data.Subset(emnist_train_dataset, filtered_train_indices)\n",
        "\n",
        "filtered_test_indices = [i for i in range(len(emnist_test_dataset)) if emnist_test_dataset.targets[i] in target_labels]\n",
        "test_data = torch.utils.data.Subset(emnist_test_dataset, filtered_test_indices)\n"
      ],
      "metadata": {
        "id": "yabNQ4oO7Pyd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define train and test dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "MDFqWqDNAB_G"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VISUALIZATION"
      ],
      "metadata": {
        "id": "JfK-W8HJ0wmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a counter variable for visualization\n",
        "counter = 0"
      ],
      "metadata": {
        "id": "-IBvEzkg-SFu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a sample image after binarization and filtration\n",
        "sample_image, sample_label = test_data[counter]\n",
        "counter += 1\n",
        "sample_image = sample_image.squeeze().numpy()\n",
        "mapping = [\"A\", \"B\", \"C\", \"D\"]\n",
        "label_string = mapping[sample_label - 10]\n",
        "\n",
        "plt.imshow(sample_image, cmap=\"gray\")\n",
        "plt.title(f\"Label: {label_string}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "NSiHUGkk7QxU",
        "outputId": "f5f3267d-05bb-4a3f-ca7b-dbf604fc2b03"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL CLASS DEFINITION"
      ],
      "metadata": {
        "id": "r1CQDRNHSZBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "class ABCDClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ABCDClassifier, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(INPUT_CHANNELS, CONV1_CHANNELS, kernel_size=CONV_KERNEL_SIZE)\n",
        "    self.conv2 = nn.Conv2d(CONV1_CHANNELS, CONV2_CHANNELS, kernel_size=CONV_KERNEL_SIZE)\n",
        "    self.conv2_drop = nn.Dropout2d(p=DROPOUT_RATE)\n",
        "    self.fc1 = nn.Linear(FCL1_CHANNELS, FCL2_CHANNELS)\n",
        "    self.fc2 = nn.Linear(FCL2_CHANNELS, OUTPUT_CLASSES)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), MAX_POOL_KERNEL_SIZE))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), MAX_POOL_KERNEL_SIZE))\n",
        "    x = x.view(-1, FCL1_CHANNELS)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, p=DROPOUT_RATE, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return F.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "BfKt1jhuIvL4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAIN AND TEST FUNCTION DEFINITION"
      ],
      "metadata": {
        "id": "66NbyTcWTOu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = ABCDClassifier().to(device)\n",
        "\n",
        "# Define the optimizer and the loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the train function\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    converted_indices = target - 10\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = loss_fn(output, converted_indices)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx == 0:\n",
        "      print(f\"Train Epoch: {epoch}\")\n",
        "    if batch_idx % 20 == 0:\n",
        "      print(f\"[{batch_idx * len(data)} / {len(train_loader.dataset)} ({100.0 * batch_idx * len(data)/ len(train_loader.dataset):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "\n",
        "# Define the test function\n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      converted_indices = target - 10\n",
        "      output = model(data)\n",
        "      test_loss += loss_fn(output, converted_indices).item()\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(converted_indices.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print(f\"\\nTest Set: Average loss: {test_loss:.4f}, Accuracy {correct}/{len(test_loader.dataset)} ({100.0 * correct / len(test_loader.dataset):.0f}%)\\n\")"
      ],
      "metadata": {
        "id": "V71qZzgpPM0R"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING THE MODEL"
      ],
      "metadata": {
        "id": "yIVAyJkhThQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test 10 epochs\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  train(epoch)\n",
        "  test()\n",
        "\n",
        "# Determine the parameter count of the model\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model Parameter Count: {num_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NeZP_njUBYx",
        "outputId": "bfd91bc0-bbac-4296-b68a-acb040737258"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1\n",
            "[0 / 9600 (0%)]\tLoss: 1.385758\n",
            "[2000 / 9600 (21%)]\tLoss: 1.166502\n",
            "[4000 / 9600 (42%)]\tLoss: 0.908033\n",
            "[6000 / 9600 (62%)]\tLoss: 0.876347\n",
            "[8000 / 9600 (83%)]\tLoss: 0.842848\n",
            "\n",
            "Test Set: Average loss: 0.0081, Accuracy 1492/1600 (93%)\n",
            "\n",
            "Train Epoch: 2\n",
            "[0 / 9600 (0%)]\tLoss: 0.818675\n",
            "[2000 / 9600 (21%)]\tLoss: 0.810343\n",
            "[4000 / 9600 (42%)]\tLoss: 0.815025\n",
            "[6000 / 9600 (62%)]\tLoss: 0.812238\n",
            "[8000 / 9600 (83%)]\tLoss: 0.823218\n",
            "\n",
            "Test Set: Average loss: 0.0079, Accuracy 1528/1600 (96%)\n",
            "\n",
            "Train Epoch: 3\n",
            "[0 / 9600 (0%)]\tLoss: 0.813530\n",
            "[2000 / 9600 (21%)]\tLoss: 0.796925\n",
            "[4000 / 9600 (42%)]\tLoss: 0.772125\n",
            "[6000 / 9600 (62%)]\tLoss: 0.833088\n",
            "[8000 / 9600 (83%)]\tLoss: 0.803852\n",
            "\n",
            "Test Set: Average loss: 0.0079, Accuracy 1535/1600 (96%)\n",
            "\n",
            "Train Epoch: 4\n",
            "[0 / 9600 (0%)]\tLoss: 0.808162\n",
            "[2000 / 9600 (21%)]\tLoss: 0.765743\n",
            "[4000 / 9600 (42%)]\tLoss: 0.778205\n",
            "[6000 / 9600 (62%)]\tLoss: 0.783839\n",
            "[8000 / 9600 (83%)]\tLoss: 0.784179\n",
            "\n",
            "Test Set: Average loss: 0.0078, Accuracy 1545/1600 (97%)\n",
            "\n",
            "Train Epoch: 5\n",
            "[0 / 9600 (0%)]\tLoss: 0.807322\n",
            "[2000 / 9600 (21%)]\tLoss: 0.775514\n",
            "[4000 / 9600 (42%)]\tLoss: 0.782641\n",
            "[6000 / 9600 (62%)]\tLoss: 0.767894\n",
            "[8000 / 9600 (83%)]\tLoss: 0.787734\n",
            "\n",
            "Test Set: Average loss: 0.0077, Accuracy 1551/1600 (97%)\n",
            "\n",
            "Train Epoch: 6\n",
            "[0 / 9600 (0%)]\tLoss: 0.780416\n",
            "[2000 / 9600 (21%)]\tLoss: 0.794099\n",
            "[4000 / 9600 (42%)]\tLoss: 0.780087\n",
            "[6000 / 9600 (62%)]\tLoss: 0.755857\n",
            "[8000 / 9600 (83%)]\tLoss: 0.765381\n",
            "\n",
            "Test Set: Average loss: 0.0077, Accuracy 1555/1600 (97%)\n",
            "\n",
            "Train Epoch: 7\n",
            "[0 / 9600 (0%)]\tLoss: 0.779421\n",
            "[2000 / 9600 (21%)]\tLoss: 0.758923\n",
            "[4000 / 9600 (42%)]\tLoss: 0.796872\n",
            "[6000 / 9600 (62%)]\tLoss: 0.749907\n",
            "[8000 / 9600 (83%)]\tLoss: 0.760848\n",
            "\n",
            "Test Set: Average loss: 0.0077, Accuracy 1557/1600 (97%)\n",
            "\n",
            "Train Epoch: 8\n",
            "[0 / 9600 (0%)]\tLoss: 0.763864\n",
            "[2000 / 9600 (21%)]\tLoss: 0.757412\n",
            "[4000 / 9600 (42%)]\tLoss: 0.747360\n",
            "[6000 / 9600 (62%)]\tLoss: 0.755871\n",
            "[8000 / 9600 (83%)]\tLoss: 0.759600\n",
            "\n",
            "Test Set: Average loss: 0.0077, Accuracy 1560/1600 (98%)\n",
            "\n",
            "Train Epoch: 9\n",
            "[0 / 9600 (0%)]\tLoss: 0.787533\n",
            "[2000 / 9600 (21%)]\tLoss: 0.760659\n",
            "[4000 / 9600 (42%)]\tLoss: 0.756955\n",
            "[6000 / 9600 (62%)]\tLoss: 0.745197\n",
            "[8000 / 9600 (83%)]\tLoss: 0.771172\n",
            "\n",
            "Test Set: Average loss: 0.0077, Accuracy 1563/1600 (98%)\n",
            "\n",
            "Train Epoch: 10\n",
            "[0 / 9600 (0%)]\tLoss: 0.782729\n",
            "[2000 / 9600 (21%)]\tLoss: 0.755669\n",
            "[4000 / 9600 (42%)]\tLoss: 0.781759\n",
            "[6000 / 9600 (62%)]\tLoss: 0.756899\n",
            "[8000 / 9600 (83%)]\tLoss: 0.776873\n",
            "\n",
            "Test Set: Average loss: 0.0076, Accuracy 1565/1600 (98%)\n",
            "\n",
            "Model Parameter Count: 79428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAVING AND DOWNLOADING THE MODEL STATE DICTIONARY"
      ],
      "metadata": {
        "id": "TkvWZV3WToOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model state dict\n",
        "torch.save(model.state_dict(), \"image_classifier.pth\")"
      ],
      "metadata": {
        "id": "HC_j-FZvkXIB"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model state dict\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"image_classifier.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wF5TuTCulY1x",
        "outputId": "9ca56fc5-c8bc-485a-a7d2-332556179692"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e49ad02a-aab5-4870-acc1-f958dc1db2d3\", \"image_classifier.pth\", 321004)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}